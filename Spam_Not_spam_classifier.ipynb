{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxeWmZ0pVj8nj2E21pNIv5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaywhyCoder/spam_classifier/blob/main/Spam_Not_spam_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Libraries"
      ],
      "metadata": {
        "id": "UfmxHiVZlE48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "N4XWdF-gk312"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Dataset"
      ],
      "metadata": {
        "id": "gO3g52Aslkal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('Deysi/spam-detection-dataset')"
      ],
      "metadata": {
        "id": "7yFoyiqdsuFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9q3JK2OtdLp",
        "outputId": "bb407490-8aaa-47c3-dd37-c9e854ad99a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 8175\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 2725\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pre-Trained Tokenizer and Model"
      ],
      "metadata": {
        "id": "Fki2GKTPthy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYWXEpN3k3yO",
        "outputId": "f1e3d27d-f43a-4547-85e4-0cf9f07f4328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the Dataset"
      ],
      "metadata": {
        "id": "u2ESfUv2vFEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tokens(data):\n",
        "  \"\"\"\n",
        "  A function to tokenize a dataset, truncate and pad the sequence to max length\n",
        "\n",
        "  Return: Tensorflow tensors\n",
        "  \"\"\"\n",
        "  return tokenizer(data['text'], padding='max_length', truncation=True, return_tensors='tf')\n",
        "\n",
        "encoded_dataset = dataset.map(build_tokens, batched=True)"
      ],
      "metadata": {
        "id": "-yypJWMUk3vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare the Dataset"
      ],
      "metadata": {
        "id": "lk6yDhjdxTOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset = encoded_dataset.remove_columns(['text'])\n",
        "encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset = encoded_dataset['train'].shuffle(seed=42).select(range(3000))\n",
        "test_dataset = encoded_dataset['test'].shuffle(seed=42).select(range(1000))"
      ],
      "metadata": {
        "id": "XDveK4V_k3tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset):\n",
        "  \"\"\"\n",
        "  a function to convert input dataset into a TensorFlow dataset\n",
        "\n",
        "  Return: Batched Tensorflow dataset\n",
        "  \"\"\"\n",
        "  dataset = dataset.map(lambda x: {'labels': 1 if x['labels'] == 'spam' else 0})\n",
        "  return tf.data.Dataset.from_tensor_slices(({key: dataset[key] for key in dataset.features if key != \"labels\"}, dataset['labels'])).batch(8)\n",
        "\n",
        "train_tf_dataset = prepare_dataset(train_dataset)\n",
        "test_tf_dataset = prepare_dataset(test_dataset)"
      ],
      "metadata": {
        "id": "YiBw3CT7k3rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compile the Model"
      ],
      "metadata": {
        "id": "rpzZx1f12XLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "metadata": {
        "id": "z76vzbBak3og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the Model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZSWfJjL441rs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tla4QXnk1oY",
        "outputId": "c8f53f5e-1435-44ac-9a57-57b9a2d65529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375/375 [==============================] - 220s 513ms/step - loss: 0.0393 - accuracy: 0.9857 - val_loss: 0.0128 - val_accuracy: 0.9970\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7a93000d7050>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model.fit(train_tf_dataset, validation_data=test_tf_dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing the Model"
      ],
      "metadata": {
        "id": "ISGY0jD5RCf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def spam_filter(email):\n",
        "  token = build_tokens({'text': [email]})\n",
        "  pred = model.predict(token)\n",
        "  pred = np.argmax(pred.logits, axis=1)\n",
        "\n",
        "  if pred == 1:\n",
        "    print(\"Spam\")\n",
        "  else:\n",
        "    print(\"Not spam\")\n",
        "\n",
        "spam_filter(\"\"\"\n",
        "WINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LL99dyz5Np2",
        "outputId": "8f29f74a-e2ec-46ce-8052-cd1e471ac9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bu5i8U3LTEtO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}